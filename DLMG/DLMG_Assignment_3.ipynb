{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with MXNet Gluon - Assignment 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Description\n",
    "\n",
    "\n",
    "Welcome to Deep Learning with MXNet/Gluon Week 3 assignment. This assignment will focus natural language processing and using gluon-nlp. In the first question, you will answer questions about RNNs and LSTMs. Then you will do some NLP specific processing tasks and you will also get the opportunity to try out some pretrained word-embeddings in gluonnlp. Finally you will combine word embeddings and finetune an image classification model on a new dataset and train an object detection model on a dataset.\n",
    "\n",
    "### Supplemental Reading\n",
    "* [Deep Learning, NLP, Representations (Blog)](https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)\n",
    "* [Recurrent Neural Networks (Dive into deep learning)](https://d2l.ai/chapter_recurrent-neural-networks/rnn.html)\n",
    "* [LSTM (Dive into deep learning)](https://d2l.ai/chapter_recurrent-neural-networks/lstm.html)\n",
    "* [Natural Language Processing(Dive into deep learning)](https://d2l.ai/chapter_natural-language-processing/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gluonnlp in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (0.6.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from gluonnlp) (1.14.5)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gluonnlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "We saw that feed forward neural networks are not very good for modelling data that is sequential. In order to explicitly model patterns in sequential data we introduced Recurrent Neural Networks (RNNs). The hidden state in RNNs allows us to capture historical information of the sequence up to the current time step. Now, you will walk through an exercise of how this works, using a handcrafted example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Write code that performs a single RNN update given an input and the current state. \n",
    "\n",
    "The input `X` and state `H` have been initialized for you below. So have the weights. Recall that the rnn cell simply updates the hidden state by performing\n",
    "\n",
    "$$ H = \\sigma(X \\cdot W_{xh} + H \\cdot W_{hh})$$\n",
    "\n",
    "and produces the output by performing\n",
    "\n",
    "$$ O = \\sigma(X \\cdot W_{hq})$$\n",
    "\n",
    "where $\\sigma$ is the activation function. Try using both `nd.sigmoid` and `nd.relu` as the activation function. What differences do you observe. Run the rnn function for 10 time steps, feeding the output of one time step as the input to the next. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[0.       ]\n",
      " [2.331464 ]\n",
      " [0.4035033]]\n",
      "<NDArray 3x1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "from mxnet import nd\n",
    "\n",
    "# Data X and hidden state H\n",
    "X = nd.random.normal(shape=(3, 1))\n",
    "H = nd.random.normal(shape=(3, 2))\n",
    "\n",
    "# Weights\n",
    "W_xh = nd.random.normal(shape=(1, 2))\n",
    "W_hh = nd.random.normal(shape=(2, 2))\n",
    "W_hq = nd.random.normal(shape=(2, 1))\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an LSTM Language Model\n",
    "\n",
    "Now we will train an LSTM language model but with a model we designed by hand. First we load and prepare the training dataset. Similar to the example in lecture, we will be using the 'wikitext-2' dataset. We will create a training dataloader using a batched version of the dataset with the `nlp.data.batchify.CorpusBPTTBatchify` function as in lecture, so that our model can train in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import glob\n",
    "import time\n",
    "import math\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet.gluon.utils import download\n",
    "\n",
    "import gluonnlp as nlp\n",
    "\n",
    "dataset_name = 'wikitext-2'\n",
    "train_dataset, val_dataset, test_dataset = (nlp.data.WikiText2(segment=segment,\n",
    "                                                               bos=None, \n",
    "                                                               eos='<eos>', \n",
    "                                                               skip_empty=False)\n",
    "                                            for segment in ['train', 'val', 'test'])\n",
    "\n",
    "num_gpus = 1\n",
    "context = mx.gpu(0)\n",
    "log_interval = 200\n",
    "\n",
    "batch_size = 20\n",
    "bptt = 35\n",
    "\n",
    "vocab = nlp.Vocab(nlp.data.Counter(train_dataset), padding_token=None, bos_token=None)\n",
    "bptt_batchify = nlp.data.batchify.CorpusBPTTBatchify(vocab, bptt, batch_size, last_batch='discard')\n",
    "train_data, val_data, test_data = (bptt_batchify(x) for x in [train_dataset, val_dataset, test_dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "In the example in lecture, we used a standard LSTM model from gluon NLP for this assignment, we will write the LSTM model by extending `gluon.HybridBlock`. As in assignment one, the `__init__` method has been written for you and you simply need to write the `hybrid_forward` method with the signature provided.\n",
    "\n",
    "The forward method should consist of the following steps in order.\n",
    "* Encoder on input, with dropout after\n",
    "* LSTMcell. `self.rnn` in the code with dropout on the output of the LSTM cell\n",
    "* Decoder on output of LSTM cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNModel(\n",
      "  (drop): Dropout(p = 0.5, axes=())\n",
      "  (encoder): Embedding(33278 -> 650, float32)\n",
      "  (rnn): LSTM(650 -> 650, TNC, num_layers=2, dropout=0.5)\n",
      "  (decoder): Dense(650 -> 33278, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from mxnet.gluon import nn, rnn\n",
    "\n",
    "class RNNModel(gluon.HybridBlock):\n",
    "    \"\"\"A model with an encoder, recurrent layer, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, num_embed, num_hidden,\n",
    "                 num_layers, dropout=0.5, tie_weights=False, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.drop = nn.Dropout(dropout)\n",
    "            self.encoder = nn.Embedding(vocab_size, num_embed,\n",
    "                                        weight_initializer=mx.init.Uniform(0.1))\n",
    "            self.rnn = rnn.LSTM(num_hidden, num_layers, dropout=dropout,\n",
    "                                input_size=num_embed)\n",
    "            \n",
    "            if tie_weights:\n",
    "                self.decoder = nn.Dense(vocab_size, in_units=num_hidden,\n",
    "                                        params=self.encoder.params)\n",
    "            else:\n",
    "                self.decoder = nn.Dense(vocab_size, in_units=num_hidden)\n",
    "\n",
    "            self.num_hidden = num_hidden\n",
    "            \n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)\n",
    "\n",
    "    def hybrid_forward(self, F, inputs, hidden):\n",
    "        # Your code here\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "lr = 20\n",
    "model = RNNModel(len(vocab), 650, 650, 2, 0.5)\n",
    "print(model)\n",
    "\n",
    "# Your code here\n",
    "model.initialize(mx.init.Xavier(), ctx=context)\n",
    "trainer = gluon.Trainer(model.collect_params(), 'sgd',\n",
    "                        {'learning_rate': lr,\n",
    "                         'momentum': 0,\n",
    "                         'wd': 0})\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Using the example from lecture as an inspiration, write the training function to train the custom language model that we've built. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach(hidden):\n",
    "    if isinstance(hidden, (tuple, list)):\n",
    "        hidden = [i.detach() for i in hidden]\n",
    "    else:\n",
    "        hidden = hidden.detach()\n",
    "    return hidden\n",
    "\n",
    "def eval(data_source):\n",
    "    total_L = 0.0\n",
    "    ntotal = 0\n",
    "    hidden = model.begin_state(func=mx.nd.zeros, batch_size=batch_size, ctx=context)\n",
    "    for i, (data, target) in enumerate(data_source):\n",
    "        data = data.as_in_context(context)\n",
    "        target = target.as_in_context(context).reshape((-1, 1))\n",
    "        output, hidden = model(data, hidden)\n",
    "        L = loss(output, target)\n",
    "        total_L += mx.nd.sum(L).asscalar()\n",
    "        ntotal += L.size\n",
    "    return total_L / ntotal\n",
    "\n",
    "grad_clip = 0.25\n",
    "epochs = 3\n",
    "\n",
    "# Your code here: write training \n",
    "\n",
    "def train(model, train_data, val_data, test_data, epochs, lr):\n",
    "   \n",
    "    \n",
    "train(model, train_data, val_data, test_data, epochs, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analogies via Embeddings\n",
    "In class you saw an example of an application of embeddings to get word similarity. Now we will extend this by applying word embeddings to complete word analogies. Because the vector space that the word embeddings live in capture distributional semantics for the words, we can use well trained word embeddigns to perform word analogy tasks. For example, if I asked you "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 \n",
    "\n",
    "We will create a gluon-nlp embedding with the glove dataset using the `'glove.6B.50d'` source and also create a gluon-nlp `Vocab` that uses that embeddding in the `get_top_k_by_analogy` function that you will implement below.\n",
    "\n",
    "Recall, that to find the word that completes an analogy like `a:b::c:?`. You need to find the word who's embedding is the closest in cosine similarity to the vector given by `vocab.embedding[a] - vocab.embedding[b] + vocab.embedding[c]`.\n",
    "\n",
    "Check out the following methods in `gluonnlp.Vocab` and `gluonnlp.embedding` that may be helpful in your implementation: `gluonnlp.Vocab.set_embedding`, `gluonnlp.Vocab.to_tokens`, and `gluonnlp.embedding.idx_to_vec`. \n",
    "\n",
    "Try different values for `k` so you can see what other words could potentially complete the analogy according to the embedding. What do you observe? Try other word analogies you can think of and report what you observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_6b50d = nlp.embedding.create('glove', source='glove.6B.50d') # Your code here\n",
    "vocab = nlp.Vocab(nlp.data.Counter(glove_6b50d.idx_to_token)) # Your code here\n",
    "\n",
    "vocab.set_embedding(glove_6b50d)\n",
    "\n",
    "\n",
    "def get_top_k_by_analogy(vocab, word1, word2, word3, k=1):\n",
    "# Your code here\n",
    "\n",
    "print(get_top_k_by_analogy(vocab, 'man', 'woman', 'son'))\n",
    "print(get_top_k_by_analogy(vocab, 'london', 'england', 'berlin'))\n",
    "print(get_top_k_by_analogy(vocab, 'france', 'crepes', 'argentina'))\n",
    "print(get_top_k_by_analogy(vocab, 'argentina', 'football', 'india'))\n",
    "print(get_top_k_by_analogy(vocab,'bad', 'worst', 'big'))\n",
    "print(get_top_k_by_analogy(vocab, 'do', 'did', 'go'))\n",
    "print(get_top_k_by_analogy(vocab, 'argentina', 'messi', 'france', k=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
