{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with MXNet Gluon - Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Description\n",
    "\n",
    "\n",
    "Welcome to Deep Learning with MXNet/Gluon Week 1 assignment. This assignment will focus mainly on review of some prerequisites and the fundamentals of training neural networks. The first two questions will refresh our linear algebra with the help of `mxnet.ndarray`, and differential calculus with the help of `mxnet.autograd`. In the remaining questions, you will train a deep learning model to predict category of item of clothing from images. You will make use of the `gluon.Dataset` and `gluon.DataLoader` library for managing the training data, `gluon.HybridBlock` for creating your neural network model, and finally `gluon.Trainer` for training the model on your dataset.\n",
    "\n",
    "### Supplemental Reading\n",
    "* [Gluon Crash Course Videos](https://www.youtube.com/playlist?list=PLkEvNnRk8uVmVKRDgznk3o3LxmjFRaW7s)\n",
    "* [Linear Regression (Dive into deep learning)](https://d2l.ai/chapter_linear-networks/index.html)\n",
    "* [Multilayer Perceptron (Dive into deep learning)](https://d2l.ai/chapter_multilayer-perceptrons/index.html)\n",
    "* [Deep Learning Computation (Dive into deep learning)](https://d2l.ai/chapter_deep-learning-computation/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data', header = None, sep='\\s+')\n",
    "data.columns = ['CRIM','ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX','PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "X_train = data.loc[:, data.columns[:len(data.columns)-1]].values\n",
    "y_train = data.loc[:, data.columns[-1]].values\n",
    "\n",
    "def solve_ols_numpy(X, y):\n",
    "    X = np.hstack((X, np.ones((data.shape[0], 1))))\n",
    "    return np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares Linear Regression\n",
    "\n",
    "You are probably familiar with linear regression and maybe have used libraries like sklearn or numpy to implement linear regression models. Here, we will implement the analytic solution to least squares linear regression using mxnet `ndarray`. As you learned in lecture today, mxnet `ndarray` is similar to numpy `ndarray` and has support for performing matrix operations.\n",
    "\n",
    "The linear regression approach to modelling is to make the assumption that the relationship between your target variable (label) and your input variables (data) is linear. This relationship can be expressed mathematically, as\n",
    "\n",
    "$$ \\hat{y} = \\mathtt{Xw + w_0}$$\n",
    "\n",
    "where $\\mathtt{X}$ is your data, $\\hat{y}$ is your predicted label, $\\mathtt{w}$ is a vector of weights or coefficients for each column in the dataset, and $\\mathtt{w_0}$ is the intercept or bias.\n",
    "\n",
    "\n",
    "If we let $ X = [\\mathtt{X}, \\mathtt{1}]$ and $w = [\\mathtt{w, w_0}]$ then we can rewrite the above equation and simplify to just a simple matrix multiplication as \n",
    "\n",
    "$$ \\hat{y} = Xw$$\n",
    "\n",
    "\n",
    "To find the value of the weights $w$, we have to solve an optimization problem involving a loss function that tells how good our predictions are. The loss function we will be using is the least squares loss or l2 loss and it is given as\n",
    "    $$f(w) = ||Xw - y||^2$$ \n",
    "where $||.||$ denotes the $L_2$ norm and $y$ is your actual label value.\n",
    "\n",
    "The $w$ that minimizes the function above has a closed form solution and it is given as:\n",
    "\n",
    "$$ w = (X^TX)^{-1}X^Ty $$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 \n",
    "Write code to calculate the value of $w$ using the close form solution above given the data matrix $\\mathtt{X}$ and data labels $y$ using mxnet `ndarray`. It might make things easier if you organize your code in a function like `solve_ols_numpy` above. Here are some guidelines to help with the implementation.\n",
    "\n",
    "* First create `mxnet.ndarray` representations of X_train and y_train\n",
    "* Don't forget to create an `mxnet.ndarray` column vector of ones, for the intercept, with the same number of rows as X_train and concatenate X_train with the column vector of ones\n",
    "* Use the `mxnet.nd.linalg.potri` and `mxnet.nd.linalg.potrf` functions to compute the inverse. For example to compute the inverse of a matrix M, you can run `nd.linalg.potri(nd.linalg.potrf(M))`. MXNet does not have a generic matrix inverse operator because all operators need to be differentiable, but luckily our Gramian matrix $X^TX$ is symmetric and positive definite so we can use the [cholesky decomposition](https://en.wikipedia.org/wiki/Cholesky_decomposition) to compute its inverse.\n",
    "\n",
    "Compare your results with the results of using the pure numpy `solve_ols_numpy` function defined above.\n",
    "\n",
    "Try timing your code and then changing the contexts of your mxnet ndarrays to gpu. Did you notice any speed difference. Can you explain the difference in speed? Also time how long `solve_ols_numpy` takes to run. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 1 \n",
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 1\n",
    "\n",
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent with `autograd`\n",
    "\n",
    "Earlier, we saw how to use `mxnet.ndarray` operations to solve ordinary least squares linear regression. Luckily we were able to do this because a closed-form solution exists for minimizing our loss function. In general, we are not always so lucky that a closed form solution exists. For example if we modify the loss function above slightly to include an $L_1$ regularization term like \n",
    "\n",
    "$$f(w) = \\frac{1}{n}||Xw - y||_2^2 + \\lambda||w||_1$$\n",
    "\n",
    "then no closed form solution exists. \n",
    "\n",
    "Instead, we can turn to a trusted general-purpose algorithm for minimizing any arbitrary function that has a derivative - gradient descent. gradient descent is an iterative algorithm that starts with an initialization for the values of our weights and successively refines the values of the weights by moving them in the direction of the negative gradient. \n",
    "\n",
    "Take a look again at the loss function above. Can you compute the derivative for it? If your answer is no, then no need to fret. That's what `autograd` is here for. We will implement gradient descent to solve the $L_1$ regularized version of OLS using only `mnxet.ndarray` and `mxnet.autograd` and you won't have to compute any derivatives by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Write code to calculate the value of $w$ using minibatch gradient descent given the data matrix $\\mathtt{X}$ and data labels $y$.  Here are some guidelines to help with the implementation.\n",
    "\n",
    "* Fill in the code for the loss function which simply computes the $L_1$ regularized loss above\n",
    "* Fill in the code for the optimize function which performs a single gradient descent update step. Make sure the weight changes are in place. Also normalize the gradient by the batch-size\n",
    "* Fill in the code for the train function using the default parameters in the argument.\n",
    "    * Initialize w with a standard normal distribution\n",
    "    * In your training loop, use the autograd scope to record the computations need to compute the loss so you can compute the gradient\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import autograd\n",
    "\n",
    "def loss_fn(X, w, y, lambd):\n",
    "    # Your code here\n",
    "    \n",
    "\n",
    "def optimize(w, lr, batch_size):\n",
    "    # Your code here\n",
    "    \n",
    "\n",
    "def train(X, y, lambd=0.1, lr=0.001, batch_size=506, epochs=50, ctx=mx.cpu()):\n",
    "    loss_sequence = []\n",
    "    \n",
    "    # Your code here\n",
    "    \n",
    "        \n",
    "    # end implementation\n",
    "    return loss_sequence\n",
    "\n",
    "losses = train(X_train, y_train)\n",
    "\n",
    "plt.figure(num=None,figsize=(8, 6))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('iteration',fontsize=14)\n",
    "plt.ylabel('Mean loss',fontsize=14)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 2\n",
    "\n",
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion MNIST\n",
    "\n",
    "The most commonly used image classification data set is the MNIST handwritten digit recognition data set, proposed by LeCun, Cortes and Burges in the 1990s. However, even simple (shallow) models achieve classification accuracy over 95% on MNIST. In order to get a better intuition, we will use the qualitatively similar, but comparatively complex Fashion-MNIST dataset, proposed by Xiao, Rasul and Vollgraf in 2017. The goal is no longer to classify numbers, but clothing types instead.\n",
    "\n",
    "The dataset can be automatically downloaded through Gluon's data.vision.datasets module. The following code downloads the training dataset and shows the first example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "from mxnet.gluon.data.vision import datasets\n",
    "\n",
    "fashion_mnist_train = datasets.FashionMNIST(train=True)\n",
    "fashion_mnist_valid = datasets.FashionMNIST(train=False)\n",
    "\n",
    "print(\"Number of images: %d\" % len(fashion_mnist_train))\n",
    "\n",
    "text_labels = [\n",
    "    't-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "    'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot'\n",
    "]\n",
    "def plot_images(X, y):\n",
    "    # plot images\n",
    "    _, figs = plt.subplots(1, X.shape[0], figsize=(15, 15))\n",
    "    for f,x,yi in zip(figs, X,y):\n",
    "        # 3D->2D by removing the last channel dim\n",
    "        f.imshow(x.reshape((28,28)).asnumpy())\n",
    "        ax = f.axes\n",
    "        ax.set_title(text_labels[int(yi)])\n",
    "        ax.title.set_fontsize(20)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "    \n",
    "X, y = fashion_mnist_train[0:6]\n",
    "plot_images(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "The training and validation dataset and stored into a gluon `dataset`. However, in order to get good performance on our dataset, we must do some preprocessing on the data. For example, We will have to normalize the image data.\n",
    "\n",
    "Write code to using `transforms.Compose` to convert the images to tensor representation in `CHW` format and then normalize the dataset. Take a look at [ToTensor](https://mxnet.incubator.apache.org/api/python/gluon/data.html#mxnet.gluon.data.vision.transforms.ToTensor) and [Normalize](https://mxnet.incubator.apache.org/api/python/gluon/data.html#mxnet.gluon.data.vision.transforms.Normalize) to learn how to do this. To normalize, use the `normalize_mean` and `normalize_stdev` variables provided below. \n",
    "\n",
    "Next, write code to create a `gluon.data.DataLoader` that's able to take batches of data from your transformed dataset and feed into your network. Use the batch_size variable provided below. Make sure the **training** dataloader shuffles the data at each epoch.\n",
    "\n",
    "After transforming your dataset, what is the shape of each training image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon.data.vision import transforms\n",
    "batch_size = 256\n",
    "normalize_mean = 0.13\n",
    "normalize_stdev = 0.31\n",
    "\n",
    "# Your code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet\n",
    "\n",
    "Now it's time to construct our neural network architecture. Since we are dealing with image data we will get better performance on this dataset with a convolutional neural network. We will go into the details of designing Convolutional Neural Networks in the next class but for now we will be using a specific network known as [LeNet](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf) network architeture introduced by LeCun et al for the handwritten digit recognition on the MNIST data set. The network has the following structure: 2 convolutional blocks, 2 fully connected hidden layers and an output layer. A typical convolutonal block consists of A convolution layer, followed by the activation function and then a pooling layer. \n",
    "\n",
    "![lenet](https://cdn-images-1.medium.com/max/2400/1*1TI1aGBZ4dybR6__DI9dzA.png)\n",
    "\n",
    "The LeNet paper used `tanh` as the activation functions, but we will use `relu` here since it's easy to train and gives better performance typically. We will also use max pooling since it works better in practice as well.\n",
    "\n",
    "We will initialize the parameters of network with the Xavier initialization method which is a popular choice for convolutional neural networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Write code to compute the forward pass of the network by filling in the `hybrid_forward` function below.\n",
    "\n",
    "Here are some guidelines. \n",
    "* Remember to apply the activation function `relu` on the outputs of the convolution layer before applying the pool layer.\n",
    "* Reshape the output of the second convolutional block so that you have a flat vector that goes into your fully connected layers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn, HybridBlock\n",
    "\n",
    "class LeNet(HybridBlock):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        with self.name_scope():\n",
    "            self.conv1 = nn.Conv2D(channels=6, kernel_size=5)\n",
    "            self.pool1 = nn.MaxPool2D(pool_size=2, strides=2)\n",
    "            self.conv2 = nn.Conv2D(channels=16, kernel_size=3)\n",
    "            self.pool2 =  nn.MaxPool2D(pool_size=2, strides=2)\n",
    "            self.hidden1 = nn.Dense(120)\n",
    "            self.hidden2 = nn.Dense(84)\n",
    "            self.out = nn.Dense(10)\n",
    "    \n",
    "    def hybrid_forward(self, F, x):\n",
    "        # Your code here\n",
    "        \n",
    "\n",
    "net = LeNet()\n",
    "net.initialize(mx.init.Xavier())\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "Now, write code to perform the training loop. Use the gluon `SoftmaxCrossEntropyLoss` loss function and `sgd` optimizer with a learning rate of `0.1`. Run training for 10 epochs and plot the training and validation accuracy at each epoch.\n",
    "\n",
    "Try out different optimizers and values for the learning rate. Does your accuracy change? The learning rate and choice of optimizer are known as hyperparameters as they are parameters that are not learned by the network but can influence the values of the learned parameters and as such the performance of the network. Briefly, describe you might try to optimizer the values for the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "def acc(output, label):\n",
    "    # output: (batch, num_output) float32 ndarray\n",
    "    # label: (batch, ) int32 ndarray\n",
    "    acc = (output.argmax(axis=1) == label.astype('float32'))\n",
    "    return acc.mean().asscalar()\n",
    "\n",
    "losses, train_accuracies, validation_accuracies = [], [], []\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(num=None,figsize=(8, 6))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch',fontsize=14)\n",
    "plt.ylabel('Mean loss',fontsize=14)\n",
    "\n",
    "plt.figure(num=None,figsize=(8, 6))\n",
    "plt.plot(range(10), train_accuracies, range(10), validation_accuracies)\n",
    "plt.xlabel('Epoch',fontsize=14)\n",
    "plt.ylabel('Accuracy',fontsize=14)\n",
    "plt.legend(['train accuracy', 'validation accuracy'])\n",
    "plt.ylim([0, 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
