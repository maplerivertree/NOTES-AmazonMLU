{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RNNs,  LSTMs and Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-06T21:56:37.022037Z",
     "start_time": "2018-06-06T21:56:35.507701Z"
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import gluon, nd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "RNNs allow you to learn from sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RNN\n",
    "\n",
    "![](support/rnn-unrolled.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$h_t=𝐖_{hx}X_t+𝐖_{hh}h_{t−1}$$\n",
    "$$o_t=𝐖_{𝑜ℎ}h_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Create an rnn in gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon import rnn\n",
    "\n",
    "num_hiddens = 256\n",
    "rnn_layer = rnn.RNN(num_hiddens)\n",
    "rnn_layer.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Initialize the hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 256)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "state = rnn_layer.begin_state(batch_size=batch_size)\n",
    "state[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "the input shape of rnn_layer is given by (time step, batch size, number of inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 2, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((35, 2, 256), 1, (1, 2, 256))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_steps = 35\n",
    "X = nd.random.uniform(shape=(num_steps, batch_size, 3))\n",
    "Y, state_new = rnn_layer(X, state)\n",
    "print(X.shape)\n",
    "Y.shape, len(state_new), state_new[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The hidden state returned by the rnn.RNN instance in the forward computation is the state of the hidden layer available at the last time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## BPTT\n",
    "![](support/rnn-bptt.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Computational dependencies for a recurrent neural network model with three time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Gradients of the loss at at time t depend on results of hidden layers at previous time steps, recursively until the first time step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\nabla_{𝐖_{ℎℎ}}𝐡_𝑡 = \\sum_{𝑗=1}^𝑡(𝐖^⊤_{ℎℎ})^{𝑡−𝑗}𝐡_𝑗$$\n",
    "$$\\nabla_{𝐖_{ℎ𝑥}}𝐡_𝑡=\\sum_{𝑗=1}^𝑡(𝐖^⊤_{ℎℎ})^{𝑡−𝑗}𝐱_𝑗$$\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### BackPropagation through time.\n",
    "\n",
    "Like regular backprop but with time.\n",
    "\n",
    "* store intermediate results, i.e. powers of  $𝐖_{ℎℎ}$ and $h_j$ as we do the forward pass to compute the loss\n",
    "* truncate sum to avoid numerical issues i.e gradient detachment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Matrix power can become arbitarily large. This is numerically unstable because eigenvalues smaller than 1 vanish for large powers and eigenvalues larger than 1 explode.  One way to address this is to truncate the sum at a computationally convenient size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LSTM\n",
    "![](support/lstm-chain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Long Short Term Memory \n",
    "\n",
    "* LSTMs model long term dependencies better than RNNs\n",
    "* three types of gates that control the flow of information: input, forget and output gates .\n",
    "* The hidden layer output of LSTM includes hidden states and memory cells. \n",
    "* output is computed using only hidden state. Memory cells are entirely internal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Create an lstm in gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lstm_layer = rnn.LSTM(num_hiddens)\n",
    "lstm_layer.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Initialize the hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "state = lstm_layer.begin_state(batch_size=batch_size)\n",
    "print(len(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Language Model with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import glob\n",
    "import time\n",
    "import math\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet.gluon.utils import download\n",
    "\n",
    "import gluonnlp as nlp\n",
    "\n",
    "num_gpus = 1\n",
    "context = mx.gpu(0)\n",
    "log_interval = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dataset_name = 'wikitext-2'\n",
    "train_dataset, val_dataset, test_dataset = (nlp.data.WikiText2(segment=segment,\n",
    "                                                               bos=None, \n",
    "                                                               eos='<eos>', \n",
    "                                                               skip_empty=False)\n",
    "                                            for segment in ['train', 'val', 'test'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(size=33278, unk=\"<unk>\", reserved=\"['<eos>']\")\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "bptt = 35\n",
    "\n",
    "vocab = nlp.Vocab(nlp.data.Counter(train_dataset), padding_token=None, bos_token=None)\n",
    "print(vocab)\n",
    "\n",
    "bptt_batchify = nlp.data.batchify.CorpusBPTTBatchify(vocab, bptt, batch_size, last_batch='discard')\n",
    "train_data, val_data, test_data = (bptt_batchify(x) for x in [train_dataset, val_dataset, test_dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardRNN(\n",
      "  (embedding): HybridSequential(\n",
      "    (0): Embedding(33278 -> 200, float32)\n",
      "    (1): Dropout(p = 0.2, axes=())\n",
      "  )\n",
      "  (encoder): LSTM(200 -> 200, TNC, num_layers=2, dropout=0.2)\n",
      "  (decoder): HybridSequential(\n",
      "    (0): Dense(200 -> 33278, linear)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_name = 'standard_lstm_lm_200'\n",
    "model, vocab = nlp.model.get_model(model_name, vocab=vocab, dataset_name=None)\n",
    "print(model)\n",
    "\n",
    "model.initialize(mx.init.Xavier(), ctx=context)\n",
    "\n",
    "lr = 20\n",
    "trainer = gluon.Trainer(model.collect_params(), 'sgd', {\n",
    "    'learning_rate': lr,\n",
    "    'momentum': 0,\n",
    "    'wd': 0\n",
    "})\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "def detach(hidden):\n",
    "    if isinstance(hidden, (tuple, list)):\n",
    "        hidden = [detach(i) for i in hidden]\n",
    "    else:\n",
    "        hidden = hidden.detach()\n",
    "    return hidden\n",
    "\n",
    "def evaluate(model, data_source, batch_size, ctx):\n",
    "    total_L = 0.0\n",
    "    ntotal = 0\n",
    "    hidden = model.begin_state(\n",
    "        batch_size=batch_size, func=mx.nd.zeros, ctx=ctx)\n",
    "    for i, (data, target) in enumerate(data_source):\n",
    "        data = data.as_in_context(ctx)\n",
    "        target = target.as_in_context(ctx)\n",
    "        output, hidden = model(data, hidden)\n",
    "        hidden = detach(hidden)\n",
    "        L = loss(output.reshape(-3, -1), target.reshape(-1))\n",
    "        total_L += mx.nd.sum(L).asscalar()\n",
    "        ntotal += L.size\n",
    "    return total_L / ntotal\n",
    "\n",
    "grad_clip = 0.25\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 200/2983] loss 5.46, ppl 235.91, throughput 1608.40 samples/s\n",
      "[Epoch 0 Batch 400/2983] loss 5.45, ppl 233.75, throughput 1627.40 samples/s\n",
      "[Epoch 0 Batch 600/2983] loss 5.28, ppl 197.20, throughput 1634.64 samples/s\n",
      "[Epoch 0 Batch 800/2983] loss 5.30, ppl 199.86, throughput 1633.01 samples/s\n",
      "[Epoch 0 Batch 1000/2983] loss 5.26, ppl 193.16, throughput 1632.73 samples/s\n",
      "[Epoch 0 Batch 1200/2983] loss 5.26, ppl 191.98, throughput 1631.85 samples/s\n",
      "[Epoch 0 Batch 1400/2983] loss 5.25, ppl 191.44, throughput 1631.19 samples/s\n",
      "[Epoch 0 Batch 1600/2983] loss 5.32, ppl 203.73, throughput 1631.51 samples/s\n",
      "[Epoch 0 Batch 1800/2983] loss 5.19, ppl 178.75, throughput 1634.95 samples/s\n",
      "[Epoch 0 Batch 2000/2983] loss 5.21, ppl 182.49, throughput 1636.95 samples/s\n",
      "[Epoch 0 Batch 2200/2983] loss 5.11, ppl 165.54, throughput 1633.29 samples/s\n",
      "[Epoch 0 Batch 2400/2983] loss 5.15, ppl 171.94, throughput 1627.28 samples/s\n",
      "[Epoch 0 Batch 2600/2983] loss 5.16, ppl 174.19, throughput 1630.15 samples/s\n",
      "[Epoch 0 Batch 2800/2983] loss 5.08, ppl 161.21, throughput 1628.01 samples/s\n",
      "[Epoch 0] throughput 1630.61 samples/s\n",
      "[Epoch 0] time cost 37.88s, valid loss 5.15, valid ppl 172.96\n",
      "test loss 5.08, test ppl 161.46\n",
      "[Epoch 1 Batch 200/2983] loss 5.13, ppl 169.23, throughput 1610.66 samples/s\n",
      "[Epoch 1 Batch 400/2983] loss 5.16, ppl 173.69, throughput 1630.05 samples/s\n",
      "[Epoch 1 Batch 600/2983] loss 4.98, ppl 145.29, throughput 1629.72 samples/s\n",
      "[Epoch 1 Batch 800/2983] loss 5.03, ppl 152.76, throughput 1629.82 samples/s\n",
      "[Epoch 1 Batch 1000/2983] loss 5.01, ppl 149.77, throughput 1623.81 samples/s\n",
      "[Epoch 1 Batch 1200/2983] loss 5.02, ppl 151.08, throughput 1626.32 samples/s\n",
      "[Epoch 1 Batch 1400/2983] loss 5.03, ppl 153.53, throughput 1625.31 samples/s\n",
      "[Epoch 1 Batch 1600/2983] loss 5.10, ppl 164.51, throughput 1624.91 samples/s\n",
      "[Epoch 1 Batch 1800/2983] loss 4.98, ppl 146.05, throughput 1625.72 samples/s\n",
      "[Epoch 1 Batch 2000/2983] loss 5.02, ppl 150.96, throughput 1602.61 samples/s\n",
      "[Epoch 1 Batch 2200/2983] loss 4.92, ppl 136.86, throughput 1632.47 samples/s\n",
      "[Epoch 1 Batch 2400/2983] loss 4.96, ppl 142.79, throughput 1631.33 samples/s\n",
      "[Epoch 1 Batch 2600/2983] loss 4.98, ppl 145.88, throughput 1621.40 samples/s\n",
      "[Epoch 1 Batch 2800/2983] loss 4.91, ppl 135.01, throughput 1619.15 samples/s\n",
      "[Epoch 1] throughput 1624.03 samples/s\n",
      "[Epoch 1] time cost 38.04s, valid loss 5.05, valid ppl 156.33\n",
      "test loss 4.98, test ppl 145.88\n",
      "[Epoch 2 Batch 200/2983] loss 4.97, ppl 143.65, throughput 1605.63 samples/s\n",
      "[Epoch 2 Batch 400/2983] loss 5.00, ppl 148.19, throughput 1617.02 samples/s\n",
      "[Epoch 2 Batch 600/2983] loss 4.82, ppl 124.10, throughput 1620.63 samples/s\n",
      "[Epoch 2 Batch 800/2983] loss 4.88, ppl 131.65, throughput 1617.42 samples/s\n",
      "[Epoch 2 Batch 1000/2983] loss 4.87, ppl 130.53, throughput 1624.72 samples/s\n",
      "[Epoch 2 Batch 1200/2983] loss 4.88, ppl 131.61, throughput 1626.62 samples/s\n",
      "[Epoch 2 Batch 1400/2983] loss 4.91, ppl 135.59, throughput 1626.68 samples/s\n",
      "[Epoch 2 Batch 1600/2983] loss 4.98, ppl 145.48, throughput 1627.01 samples/s\n",
      "[Epoch 2 Batch 1800/2983] loss 4.87, ppl 129.97, throughput 1629.05 samples/s\n",
      "[Epoch 2 Batch 2000/2983] loss 4.90, ppl 134.33, throughput 1626.66 samples/s\n",
      "[Epoch 2 Batch 2200/2983] loss 4.80, ppl 121.84, throughput 1625.25 samples/s\n",
      "[Epoch 2 Batch 2400/2983] loss 4.85, ppl 127.17, throughput 1624.89 samples/s\n",
      "[Epoch 2 Batch 2600/2983] loss 4.87, ppl 130.17, throughput 1625.94 samples/s\n",
      "[Epoch 2 Batch 2800/2983] loss 4.80, ppl 121.56, throughput 1629.88 samples/s\n",
      "[Epoch 2] throughput 1624.15 samples/s\n",
      "[Epoch 2] time cost 38.03s, valid loss 4.98, valid ppl 145.29\n",
      "test loss 4.91, test ppl 136.08\n",
      "Total training throughput 1506.77 samples/s\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_data, val_data, test_data, epochs, lr):\n",
    "    best_val = float(\"Inf\")\n",
    "    start_train_time = time.time()\n",
    "    parameters = model.collect_params().values()\n",
    "    for epoch in range(epochs):\n",
    "        total_L = 0.0\n",
    "        start_epoch_time = time.time()\n",
    "        start_log_interval_time = time.time()\n",
    "        hidden = model.begin_state(batch_size, func=mx.nd.zeros, ctx=context)\n",
    "        for i, (data, target) in enumerate(train_data):\n",
    "            data = data.as_in_context(context)\n",
    "            target = target.as_in_context(context)\n",
    "            hidden = detach(hidden)\n",
    "            L = 0\n",
    "            Ls = []\n",
    "            with autograd.record():\n",
    "                output, hidden = model(data, hidden)\n",
    "                batch_L = loss(output.reshape(-3, -1), target.reshape(-1,))\n",
    "                L = L + batch_L.as_in_context(context) / (data.size)\n",
    "                Ls.append(batch_L / (data.size))\n",
    "            L.backward()\n",
    "            grads = [p.grad(data.context) for p in parameters]\n",
    "            gluon.utils.clip_global_norm(grads, grad_clip)\n",
    "\n",
    "            trainer.step(1)\n",
    "\n",
    "            total_L += sum([mx.nd.sum(l).asscalar() for l in Ls])\n",
    "\n",
    "            if i % log_interval == 0 and i > 0:\n",
    "                cur_L = total_L / log_interval\n",
    "                print('[Epoch %d Batch %d/%d] loss %.2f, ppl %.2f, '\n",
    "                      'throughput %.2f samples/s'%(\n",
    "                    epoch, i, len(train_data), cur_L, math.exp(cur_L),\n",
    "                    batch_size * log_interval / (time.time() - start_log_interval_time)))\n",
    "                total_L = 0.0\n",
    "                start_log_interval_time = time.time()\n",
    "\n",
    "        mx.nd.waitall()\n",
    "\n",
    "        print('[Epoch %d] throughput %.2f samples/s'%(\n",
    "                    epoch, len(train_data)*batch_size / (time.time() - start_epoch_time)))\n",
    "        val_L = evaluate(model, val_data, batch_size, context)\n",
    "        print('[Epoch %d] time cost %.2fs, valid loss %.2f, valid ppl %.2f'%(\n",
    "            epoch, time.time()-start_epoch_time, val_L, math.exp(val_L)))\n",
    "\n",
    "        if val_L < best_val:\n",
    "            best_val = val_L\n",
    "            test_L = evaluate(model, test_data, batch_size, context)\n",
    "            model.save_parameters('{}_{}-{}.params'.format(model_name, dataset_name, epoch))\n",
    "            print('test loss %.2f, test ppl %.2f'%(test_L, math.exp(test_L)))\n",
    "        else:\n",
    "            lr = lr*0.25\n",
    "            print('Learning rate now %f'%(lr))\n",
    "            trainer.set_learning_rate(lr)\n",
    "\n",
    "    print('Total training throughput %.2f samples/s'%(\n",
    "                            (batch_size * len(train_data) * epochs) /\n",
    "                            (time.time() - start_train_time)))\n",
    "    \n",
    "train(model, train_data, val_data, test_data, epochs, lr)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
