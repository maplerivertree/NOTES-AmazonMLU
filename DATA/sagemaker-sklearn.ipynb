{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sagemaker Skicit-learn Example\n",
    "Let's use the final project dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Get a SageMaker-compatible role used by this Notebook Instance.\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read our data from the s3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        doc_id                                               text  \\\n",
      "0  16452961621  Smells.like burnt coffee and taste disgusting....   \n",
      "1  17116740071  It was ok, but definitely not worth paying $31...   \n",
      "2  16550647171  It stings and burns under my tongue.  I have u...   \n",
      "3  17119506041  Great idea but paper does not burn uniformly n...   \n",
      "4  16969366511           Burned almost anything I tried to toast.   \n",
      "\n",
      "            date  star_rating                     title  \\\n",
      "0  9/15/18 15:18            1  Terrible taste and smell   \n",
      "1  9/17/18 15:50            1      not worth $31/bottle   \n",
      "2  9/16/18 18:26            1                It stings!   \n",
      "3  9/20/18 21:18            3                Ok product   \n",
      "4  9/11/18 17:17            1                         .   \n",
      "\n",
      "                  human_tag  \n",
      "0  Not Product Safety Issue  \n",
      "1  Not Product Safety Issue  \n",
      "2  Not Product Safety Issue  \n",
      "3  Not Product Safety Issue  \n",
      "4  Not Product Safety Issue  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bucket='mlu-data-example-test'\n",
    "data_key = 'training.csv'\n",
    "data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "\n",
    "df = pd.read_csv(data_location, header=0)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-Pre-processing:\n",
    "We will do pre-processing of our data. In pre-processing, we will:\n",
    "* Handle missing values\n",
    "* Normalize star_rating field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove rows with NaN value\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].apply(lambda x: x.lower())\n",
    "df[\"title\"] = df[\"title\"].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing star rating field\n"
     ]
    }
   ],
   "source": [
    "print(\"Normalizing star rating field\")\n",
    "df[\"star_rating\"] = (df[\"star_rating\"]-df[\"star_rating\"].min())/(df[\"star_rating\"].max()-df[\"star_rating\"].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-Training-Validation Split and Vectorizing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split our training data into training and validation subsets. We will use title, text and star_rating fields as predictor variables and human_tag is our label or taget variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X_train, X_val, y_train_text, y_val_text = train_test_split(df[[\"title\", \"text\", \"star_rating\"]], df[\"human_tag\"].values, test_size=0.3, shuffle=True)\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(y_train_text)\n",
    "y_train = le.transform(y_train_text)\n",
    "y_val = le.transform(y_val_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_title_vectorizer = TfidfVectorizer(max_features=350)\n",
    "tfidf_text_vectorizer = TfidfVectorizer(max_features=850) \n",
    "\n",
    "tfidf_title_vectorizer.fit(X_train[\"title\"].values)\n",
    "tfidf_text_vectorizer.fit(X_train[\"text\"].values)\n",
    "\n",
    "X_train_title_vectors = tfidf_title_vectorizer.transform(X_train[\"title\"].values)\n",
    "X_train_text_vectors = tfidf_text_vectorizer.transform(X_train[\"text\"].values)\n",
    "\n",
    "X_val_title_vectors = tfidf_title_vectorizer.transform(X_val[\"title\"].values)\n",
    "X_val_text_vectors = tfidf_text_vectorizer.transform(X_val[\"text\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_data = np.column_stack([X_train[\"star_rating\"].values, \n",
    "                              X_train_title_vectors.toarray(), \n",
    "                              X_train_text_vectors.toarray(),\n",
    "                              y_train\n",
    "                             ])\n",
    "\n",
    "validation_data = np.column_stack([X_val[\"star_rating\"].values, \n",
    "                              X_val_title_vectors.toarray(), \n",
    "                              X_val_text_vectors.toarray(),\n",
    "                              y_val\n",
    "                             ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-Saving the features and vectorizers into S3 bucket\n",
    "We successfully extracted features. It is now time to the data it to our S3 bucket so that our training algorithm can use it. It will be uploaded to \"processed_data\" folder. We will also save our vectorizers to our s3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Let's save our vectorizers locally\n",
    "with open(\"tfidf_title_vectorizer.pickle\", \"wb\") as f:\n",
    "    pickle.dump(tfidf_title_vectorizer, f)\n",
    "with open(\"tfidf_text_vectorizer.pickle\", \"wb\") as f:\n",
    "    pickle.dump(tfidf_text_vectorizer, f)\n",
    "\n",
    "# Save training and validation data locally\n",
    "np.save('train_data', train_data)\n",
    "np.save('validation_data', validation_data)\n",
    "\n",
    "# Upload the data to our S3 bucket\n",
    "prefix = 'processed_data'\n",
    "bucket='mlu-data-example-test'\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train_data.npy')).upload_file('train_data.npy')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation_data.npy')).upload_file('validation_data.npy')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object('tfidf_title_vectorizer.pickle').upload_file('tfidf_title_vectorizer.pickle')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object('tfidf_text_vectorizer.pickle').upload_file('tfidf_text_vectorizer.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-Training and Deployment Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the training data path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://mlu-data-example-test/processed_data/train\n"
     ]
    }
   ],
   "source": [
    "bucket = 'mlu-data-example-test'\n",
    "prefix = 'processed_data/train'\n",
    "\n",
    "train_input = 's3://{}/{}'.format(bucket, prefix)\n",
    "print(train_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a separate instance for training. \"script_path\" variable holds the file name/path for our training and inference code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "import numpy as np\n",
    "\n",
    "script_path = 'sklearn_training_burn_project.py'\n",
    "\n",
    "sklearn = SKLearn(\n",
    "    entry_point=script_path,\n",
    "    train_instance_type=\"ml.c4.xlarge\",\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command will take some time, it will print information about the instance and training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-11-14 22:39:09 Starting - Starting the training job.\n",
      "2019-11-14 22:39:18 Starting - Launching requested ML instances............\n",
      "2019-11-14 22:40:19 Starting - Preparing the instances for training.......\n",
      "2019-11-14 22:41:03 Downloading - Downloading input data.....\n",
      "2019-11-14 22:41:30 Training - Downloading the training image..\n",
      "2019-11-14 22:41:48 Training - Training image download completed. Training in progress..\n",
      "2019-11-14 22:41:59 Uploading - Uploading generated training model\n",
      "2019-11-14 22:42:04 Completed - Training job completed\n"
     ]
    }
   ],
   "source": [
    "sklearn.fit({'train': train_input}, logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will deploy our model to a \"ml.t2.medium\" instance with the endpoint name: \"endpoint-sklearn-new\". This will also take some to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "predictor = sklearn.deploy(initial_instance_count=1, instance_type=\"ml.t2.medium\", endpoint_name=\"endpoint-sklearn-test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-Test the endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = '{\"data\": [{\"text\":\"the laptop gets hot even when idling\", \"title\":\"it burnt!\", \"star_rating\":1}]}'\n",
    "sm=boto3.client(\"runtime.sagemaker\")\n",
    "response = sm.invoke_endpoint(\n",
    "      EndpointName='endpoint-sklearn-test',\n",
    "      Body=payload,\n",
    "      ContentType=\"application/json\",\n",
    "      Accept=\"application/json\")\n",
    "\n",
    "a = int(response['Body'].read().decode()[6])\n",
    "    \n",
    "if(a == 0):\n",
    "    answer = \"product safety issue\"\n",
    "else:\n",
    "    answer = \"not product safety issue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product safety issue\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
