	
// The code in this cell is generated by an Eider credential cell.
// If you import this notebook, UI controls will replace the code.
// The AWS Odin material set shown in the code below will be applied when you run the cell.
eider.odin.setAwsCredential("xxxxxxxxxxxxxxxxxxx)


import pandas as pd
import numpy as np
eider.s3.download('s3://eider-datasets/mlu/DATA_Training.csv','/tmp/DATA_Training.csv')
eider.s3.download('s3://eider-datasets/mlu/DATA_Public_Test.csv','/tmp/DATA_Public_Test.csv')
train = pd.read_csv('/tmp/DATA_Training.csv', na_values = 'null')
public_test = pd.read_csv('/tmp/DATA_Public_Test.csv', na_values = 'null')
train.head()#;public_test.head()
	
a = pd.DataFrame(list(zip(train.columns.values, train.count(axis = 0))))
b = (a[:][1] == 8347)
a = pd.DataFrame(list(zip(train.columns.values, b)))
a.columns = ["Columns", "No Missing Data"]
a

import matplotlib.pyplot as plt
train.hist(bins = 50, figsize=(22, 16), color = "k")
plt.show()


	
from pandas.plotting import scatter_matrix
 
train_set = train.copy()
 
all_attributes = ["response", "score5", "score4", "score3", "score2", "score1","hour", "prime", "contact_type", "day"]
selected_attributes = ["response", "score1","score2","score3"]
scatter_matrix(train_set[selected_attributes], figsize = (10,10), color = "k")
 
train.columns.values
corr_matrix = train_set.corr()
corr_df = pd.DataFrame(corr_matrix["response"].sort_values(ascending = False))
corr_df.columns = ["correlations to CS response"]
corr_df
#corr_matrix["score5"].sort_values(ascending = False)
#corr_matrix["score4"].sort_values(ascending = False)
#corr_matrix["score3"].sort_values(ascending = False)
#...

	
# create subset (X_train) of features that are numeric and with no missin data
X_train = train_set[["prime", "day", "hour"]][0:6000]
y_train = train_set[["response"]][0:6000]
 
X_test_pub = public_test[["prime", "day", "hour"]]
 
from sklearn import preprocessing
import numpy as np
X_scaled = preprocessing.scale(X_train)
 
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score, cross_val_predict
from sklearn.metrics import f1_score, accuracy_score
 
dt_clf = DecisionTreeClassifier(random_state = 821150)
log_clf = LogisticRegression()
rnd_clf = RandomForestClassifier(random_state = 821150)
svm_clf = SVC()
voting_clf = VotingClassifier(estimators = [ ('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)], voting = 'hard')
 
dt_clf.fit(X_scaled, y_train)
#log_clf.fit(X_train, y_train)
#rnd_clf.fit(X_train, y_train)
#svm_clf.fit(X_train, y_train)
#voting_clf.fit(X_train, y_train)
 
def evaluate(clf):
    cross_val = cross_val_score(clf, X_scaled, y_train, cv=5)
    y_train_pred = cross_val_predict(clf, X_scaled, y_train, cv = 5)
    f1 = f1_score(y_train, y_train_pred)
    a_score = accuracy_score(y_train, y_train_pred)
    print("for ", str(clf), "\naccuracy score = ",
    round(a_score, 3), "\nthe f1_score= ", 
    f1, "\ncross_val_score= ", cross_val,
    "\ncv_score_mean= ", round(cross_val.mean(),3), "\n=============================================================================" )
 
#print(np.ravel(y_train).shape)
evaluate(dt_clf)
print(dt_clf.feature_importances_)
pd.DataFrame(X_scaled)
#y_pred=[]
#for i in np.arange(len(X_test)):
#    a = dt_clf.predict(pd.DataFrame(X_test.iloc[0]).T)
#    y_pred.append(int(a))
